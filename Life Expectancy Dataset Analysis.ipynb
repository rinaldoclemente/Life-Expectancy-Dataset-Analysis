{"cells":[{"metadata":{},"cell_type":"markdown","source":"# LOAD DATASET"},{"metadata":{"_uuid":"bfaab626-9854-4789-aaf5-ec56cc285962","_cell_guid":"7cbfbec8-7e62-4e10-a83d-db1e1b8d9685","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport graphviz\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import tree","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edd0c61a-5afe-45da-b00b-d6bc9857ef5c","_cell_guid":"b9ec48ab-dbd7-480d-a688-22199b5e9fc1","trusted":true},"cell_type":"code","source":"# Load the dataset.\nle = pd.read_csv('/kaggle/input/life-expectancy-who/Life Expectancy Data.csv', sep=',')\nle.dataframeName = 'Life Expectancy Data.csv'\nle.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA PREPROCESSING\n\n# - Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modify the original names of the features using a standard format for all the features.\norig_cols = list(le.columns) \nnew_cols = [] \nfor col in orig_cols:     \n    new_cols.append(col.strip().replace('  ', ' ').replace(' ', '_').lower()) \n\nle.columns = new_cols\n\n# Compute a summary of statistics only for the numerical features.\nle.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Discard the metadata (country and year).\nle = le.drop(['country','year'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"742bab98-a960-4342-87d5-9a73884a3ad5","_cell_guid":"af7e409f-761d-43d1-b15b-dfb544b405f5","trusted":true},"cell_type":"code","source":"# For each feature count all rows with NULL values.\nle.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f3ec444-5ddd-493c-abcf-4ddf07dac1e8","_cell_guid":"7f0e6f8b-9967-4cdc-9eb6-b473f9fa89b0","trusted":true},"cell_type":"code","source":"# For each feature delete all rows with NULL values.\nle.dropna(inplace=True)\nle.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Change column order to better perform splits\nnew_order = [1,0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\nle = le[le.columns[new_order]]\nle.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# - Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get a concise summary of the dataset.\nle.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- **Box Plots**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dictionary of columns representing the features of the dataset.\ncol_dict = {'life_expectancy':1,'adult_mortality':2,'infant_deaths':3,'alcohol':4,'percentage_expenditure':5,'hepatitis_b':6,'measles':7,'bmi':8,\n            'under-five_deaths':9,'polio':10,'total_expenditure':11,'diphtheria':12,'hiv/aids':13,'gdp':14,'population':15,'thinness_1-19_years':16,\n            'thinness_5-9_years':17,'income_composition_of_resources':18,'schooling':19}\n\n# Visualize the data for each feature using box plots.\nplt.figure(figsize=(18,30))\n\nfor variable,i in col_dict.items():\n                     plt.subplot(5,4,i)\n                     plt.boxplot(le[variable],whis=1.5)\n                     plt.title(variable)\n\nplt.show()\nle.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove the outliers using the interquartile range (IQR).\nQ1 = le.quantile(0.25)\nQ3 = le.quantile(0.75)\nIQR = Q3 - Q1\n\nle = le[~((le < (Q1 - 1.5 * IQR)) |(le > (Q3 + 1.5 * IQR))).any(axis=1)]\n\n#Replace Status into boolean variables\nle[\"status\"].replace({\"Developing\": 1, \"Developed\": 0}, inplace=True)\n\n# Print the dimensions of the cleaned dataset.\nle.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the cleaned data for each feature using box plots.\nplt.figure(figsize=(18,30))\n\nfor variable,i in col_dict.items():\n                     plt.subplot(5,4,i)\n                     plt.boxplot(le[variable],whis=1.5)\n                     plt.title(variable)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- **Heatmap**"},{"metadata":{"_uuid":"6ac2748f-d2bf-4681-be47-7a0a7e17c03b","_cell_guid":"533866f5-2615-417f-a8f3-bcc3d70ac92e","trusted":true},"cell_type":"code","source":"# Plot heatmap to visualize the correlations.\nplt.figure(figsize = (14, 12))\nsns.heatmap(le.corr(), annot = True)\nplt.title('Correlation between different features');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- **Scatter Plots**"},{"metadata":{"_uuid":"7dbee9e7-3818-4082-b174-8961ddd2922c","_cell_guid":"d8ba81bb-a096-40b0-8645-54c7d158551c","trusted":true},"cell_type":"code","source":"# Create a vector containing all the features of the dataset.\nall_col = ['adult_mortality','infant_deaths','alcohol','percentage_expenditure','hepatitis_b','measles','bmi',\n         'under-five_deaths','polio','total_expenditure','diphtheria','hiv/aids','gdp','population','thinness_1-19_years',\n         'thinness_5-9_years','income_composition_of_resources','schooling']\n\nplt.figure(figsize=(15,30))\n\n# Plot each feature in function of the target variable (life_expectancy) using scatter plots.\nfor i in range(len(all_col)):\n    plt.subplot(7,3,i+1)\n    plt.scatter(le[all_col[i]], le['life_expectancy'])\n    plt.xlabel(all_col[i])\n    plt.ylabel('Life Expectancy')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1af86403-6535-4eb4-9850-8ff2cf6bf9f5","_cell_guid":"9937405d-b55c-48e1-94f9-27057fe8b095","trusted":true},"cell_type":"markdown","source":"# - Features Extraction\n- **PCA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate the features from the labels.\nX = le.iloc[:,1:].values\ny = le.iloc[:,0].values #Life Expectancy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize the data.\nX_std= StandardScaler().fit_transform(X)\nmean_vec = np.mean(X_std, axis=0)\n\n# Compute covariance matrix.\ncov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\nprint('Covariance matrix \\n%s' %cov_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute eigenvalues and eigenvectors.\neig_vals, eig_vecs = np.linalg.eig(cov_mat)\n\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the variance for every eigenvalue.\ntot = sum(eig_vals)\n\nvar_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n\nvar_exp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Plot the principal components.\nplt.figure(figsize=(10,4))\nplt.bar(range(19), var_exp, alpha=0.7, align='center', label='Individual Variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\nplt.xticks(np.arange(0, 19, 1.0))\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the cumulative variance.\npca = PCA(n_components=19).fit(X_std)\nplt.figure(figsize=(12, 4))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), label='Cumulative Variance')\nplt.xlim(0,18,1)\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')\nplt.legend(loc='best')\nplt.grid(color='#E3E3E3')\nplt.xticks(np.arange(0, 19, 1.0));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA ANALYSIS\n\n# - Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take the the values of the target variable and of the most correlated feature with the target variable.\nle_features = le['income_composition_of_resources'].values.reshape(-1,1)\nle_labels = le['life_expectancy'].values.reshape(-1,1)\n\n# Normalize the data.\nmin_max_scaler = MinMaxScaler()\nle_features = min_max_scaler.fit_transform(le_features)\n\n# Split the dataset in training and test set.\nle_features_train, le_features_test, le_labels_train, le_labels_test = train_test_split(le_features, le_labels, train_size = 0.7, test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_model = LinearRegression()\n\n# Train the model.\nlinear_model.fit(le_features_train, le_labels_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test the model.\nlinear_model_score = linear_model.predict(le_features_test)\n\n# Plot the result.\nplt.figure(figsize=(10, 6))\nplt.scatter(le_features_test, le_labels_test,  color='black')\nplt.plot(le_features_test, linear_model_score, color='blue', linewidth=2)\nplt.xlabel('Income Composition of Resources')\nplt.ylabel('Life Expectancy')\nplt.show()\n\nprint('Coefficients: \\n', linear_model.coef_)\nprint(\"Mean squared error: %.2f\" % mean_squared_error(le_labels_test,linear_model_score))\nprint(\"R^2 score : %.2f\" % r2_score(le_labels_test,linear_model_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# - Multiple Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate the features from the labels.\nle_features = le.iloc[:, 1:].values\nle_labels = le.iloc[:,0] #Life Expectancy\n\n# Normalize the data.\nmin_max_scaler = MinMaxScaler()\nle_features = min_max_scaler.fit_transform(le_features)\n\n# Split the dataset in training and test set.\nle_features_train, le_features_test, le_labels_train, le_labels_test = train_test_split(le_features, le_labels, train_size = 0.7, test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model.\nlinear_model.fit(le_features_train, le_labels_train);\n\n# Test the model.\nlinear_model_score = linear_model.predict(le_features_test)\n\nprint('Coefficients: \\n', linear_model.coef_)\nprint(\"Mean squared error: %.2f\" % mean_squared_error(le_labels_test,linear_model_score))\nprint(\"R^2 score : %.2f\" % r2_score(le_labels_test,linear_model_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# - Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the life expeectancy average\nle_avg = le['life_expectancy'].mean()\nle_lr = le.copy()\n\n# Replace 1 if life expectancy > avg, 0 otherwise\nle_lr['life_expectancy'] = (le_lr['life_expectancy'] > le_avg).astype(int)\n\n# Separate the features from the labels.\nle_features_lr = le_lr.iloc[:, 1:].values\nle_labels_lr = le_lr.iloc[:,0] #Life Expectancy\n\n# Normalize the data.\nmin_max_scaler = MinMaxScaler()\nle_features_lr = min_max_scaler.fit_transform(le_features_lr)\n\n# Split the dataset in training and test set.\nle_features_train_lr, le_features_test_lr, le_labels_train_lr, le_labels_test_lr = train_test_split(le_features_lr, le_labels_lr, train_size = 0.7, test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_model = LogisticRegression(solver='liblinear')\n\n#Train The Model\nlogistic_model.fit(le_features_train_lr, le_labels_train_lr);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_score = logistic_model.predict(le_features_test_lr)\n\n#Perform confusion matrix\nconfusion_matrix = confusion_matrix(le_labels_test_lr, logistic_score)\n\n#Print confusion matrix as heatmap\nclass_names=[0,1]\nfig,ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\nsns.heatmap(pd.DataFrame(confusion_matrix),cmap='YlGnBu',annot=True,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.xlabel('Predicted label')\nplt.ylabel('Actual label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Score on the train data: %.2f\" % logistic_model.score(le_features_train_lr, le_labels_train_lr))\nprint(\"Score on the test data: %.2f\" % logistic_model.score(le_features_test_lr, le_labels_test_lr))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# - Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform DecisionTreeRegresson with three different depths\ndecision_tree_model3 = DecisionTreeRegressor(max_depth=3)\ndecision_tree_model5 = DecisionTreeRegressor(max_depth=5)\ndecision_tree_model7 = DecisionTreeRegressor(max_depth=7)\n\n# Train the model.\ndecision_tree_model3 = decision_tree_model3.fit(le_features_train, le_labels_train)\ndecision_tree_model5 = decision_tree_model5.fit(le_features_train, le_labels_train)\ndecision_tree_model7 = decision_tree_model7.fit(le_features_train, le_labels_train)\n\nprint(\"Score on the train data with depth 3: %.2f\" % decision_tree_model3.score(le_features_train, le_labels_train))\nprint(\"Score on the test data with depth 3: %.2f\" % decision_tree_model3.score(le_features_test, le_labels_test))\nprint(\"Score on the train data with depth 5: %.2f\" % decision_tree_model5.score(le_features_train, le_labels_train))\nprint(\"Score on the test data with depth 5: %.2f\" % decision_tree_model5.score(le_features_test, le_labels_test))\nprint(\"Score on the train data with depth 7: %.2f\" % decision_tree_model7.score(le_features_train, le_labels_train))\nprint(\"Score on the test data with depth 7: %.2f\" % decision_tree_model7.score(le_features_test, le_labels_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the result.\ndot_data = tree.export_graphviz(decision_tree_model3, \n                                filled=True, \n                                rounded=True, \n                                out_file=None, \n                                feature_names=le.iloc[:, 1:].columns)\ngraph = graphviz.Source(dot_data)\ngraph","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# - Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest_model = RandomForestRegressor(n_estimators=100,\n                             min_samples_split=10,\n                             min_samples_leaf=1,\n                             max_features='auto',\n                             oob_score=True,\n                             random_state=1,\n                             n_jobs=-1)\n\n# Train the model.\nrandom_forest_model.fit(le_features_train, le_labels_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ = pd.DataFrame(le.iloc[:, 1:].columns, columns = ['feature'])\ndf_['fscore'] = random_forest_model.feature_importances_[:, ]\n\n# Plot the relative importance of the top 10 features.\ndf_['fscore'] = df_['fscore'] / df_['fscore'].max()\ndf_.sort_values('fscore', ascending = False, inplace = True)\ndf_ = df_[0:19]\ndf_.sort_values('fscore', ascending = True, inplace = True)\nax = df_.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(15, 10))\n\n# Plot the result.\nplt.title('Random forest feature importance')\nplt.xlabel('')\nplt.ylabel('')\nplt.xticks([], [])\nplt.yticks()\n\n# Create a list to collect the plt.patches data.\ntotals = []\n\n# Find the values and append to list.\nfor i in ax.patches:\n    totals.append(i.get_width())\n\n# Set individual bar lables using above list.\ntotal = sum(totals)\n\n# Set individual bar lables using above list.\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down.\n    ax.text(i.get_width(), i.get_y()+.13, \\\n            str(round((i.get_width()/total)*100, 2))+'%', fontsize=10,\ncolor='#505050')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test the model.\nrandom_forest_score = random_forest_model.predict(le_features_test)\n\nprint(\"Score on the train data: %.2f\" % random_forest_model.score(le_features_train, le_labels_train))\nprint(\"Score on the test data: %.2f\" % random_forest_model.score(le_features_test, le_labels_test))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}